{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ###### 获取token，打印token的文本，在文本中的起始位置，token的词性，token的依存关系，token是否为停止词， token是否为字母， token是否为标点符号 ###\n",
      "我 0 PRON nsubj True True False\n",
      "是 1 VERB cop True True False\n",
      "一个 2 NUM dep True True False\n",
      "小 4 ADJ amod True True False\n",
      "老虎 5 NOUN ROOT False True False\n",
      "， 7 PUNCT punct True False True\n",
      "天天 8 ADV advmod False True False\n",
      "开心 10 VERB conj False True False\n",
      "。 12 PUNCT punct True False True\n",
      "\n",
      " 13 SPACE dep False False False\n",
      "他 14 PRON nsubj True True False\n",
      "是 15 VERB cop True True False\n",
      "一个 16 X dep True True False\n",
      "【 18 PUNCT punct True False True\n",
      "大笨 19 NOUN dep False True False\n",
      "猪】 21 NOUN ROOT False False False\n",
      "， 23 PUNCT punct True False True\n",
      "走 24 VERB conj False True False\n",
      "在 25 ADP case True True False\n",
      "大路 26 NOUN nmod:prep False True False\n",
      "。 28 PUNCT punct True False True\n",
      "\n",
      " 29 SPACE dep False False False\n",
      "哈哈哈 30 PROPN nsubj False True False\n",
      "， 33 PUNCT punct True False True\n",
      "这是 34 VERB dep False True False\n",
      "一个 36 NUM dep True True False\n",
      "美妙 38 ADJ amod False True False\n",
      "的 40 PART case True True False\n",
      "清晨 41 NOUN ROOT False True False\n",
      "， 43 PUNCT punct True False True\n",
      "一切 44 PRON nsubj True True False\n",
      "都 46 ADV advmod True True False\n",
      "是 47 VERB cop True True False\n",
      "那么 48 ADV advmod True True False\n",
      "好 50 VERB amod True True False\n",
      "心情 51 NOUN conj False True False\n",
      "。 53 PUNCT punct True False True\n",
      "\n",
      " 54 SPACE dep False False False\n",
      "走 55 VERB dep False True False\n",
      "在 56 ADP case True True False\n",
      "张家界 57 PROPN nmod False True False\n",
      "的 60 PART case True True False\n",
      "山谷 61 NOUN nmod:prep False True False\n",
      "里 63 PART case False True False\n",
      "， 64 PUNCT punct True False True\n",
      "都 65 ADV advmod True True False\n",
      "是 66 VERB cop True True False\n",
      "空气 67 NOUN nsubj False True False\n",
      "美妙 69 VERB ROOT False True False\n",
      "。 71 PUNCT punct True False True\n",
      "啦 72 INTJ dep True True False\n",
      "啦 73 PART discourse True True False\n",
      "啦 74 PART discourse True True False\n",
      "啦 75 PART discourse True True False\n",
      "。 76 PUNCT ROOT True False True\n",
      "快快 77 ADV advmod False True False\n",
      "走 79 VERB dep False True False\n",
      "吧 80 PART discourse True True False\n",
      "。 81 PUNCT ROOT True False True\n",
      "\n",
      " 82 SPACE dep False False False\n",
      "快快乐乐 83 VERB dep False True False\n",
      "吧 87 PART discourse True True False\n",
      "... 88 VERB ROOT True False True\n",
      "， 91 PUNCT punct True False True\n",
      "一切 92 PRON nsubj True True False\n",
      "都 94 ADV advmod True True False\n",
      "是 95 VERB cop True True False\n",
      "快快乐乐 96 VERB conj False True False\n",
      "... 100 VERB dobj True False True\n",
      "的 103 PART discourse True True False\n",
      "吧 104 PART discourse True True False\n",
      "。 105 PUNCT punct True False True\n",
      "\n",
      " 106 SPACE dep False False False\n",
      "快 107 ADV advmod True True False\n",
      "去 108 VERB dep True True False\n",
      "吧 109 PART discourse True True False\n",
      "， 110 PUNCT punct True False True\n",
      "都 111 ADV advmod True True False\n",
      "是 112 VERB cop True True False\n",
      "好好 113 VERB dep False True False\n",
      "的 115 PART discourse True True False\n",
      "。 116 PUNCT ROOT True False True\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlpz = spacy.load('zh_core_web_sm')\n",
    "\n",
    "# doc3 = nlpz(\"我爱北京天安门\")\n",
    "# for token in doc3:\n",
    "#     print(token.text, token.pos_, token.dep_)\n",
    "\n",
    "import pathlib\n",
    "doc4 = nlpz((pathlib.Path(\"./data/text_zh.txt\").read_text(encoding='utf8')))\n",
    "\n",
    "# 获取token(文本中每个词（词组）为一个token)，打印token的文本，token的词性，token的依存关系\n",
    "print(\"\\n ###### 获取token，打印token的文本，在文本中的起始位置，token的词性，token的依存关系，token是否为停止词， token是否为字母， token是否为标点符号 ###\")\n",
    "for token in doc4:\n",
    "    print(token.text,token.idx, token.pos_, token.dep_, token.is_stop, token.is_alpha, token.is_punct)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T07:54:44.749895Z",
     "start_time": "2024-09-09T07:54:42.922598300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ###### 获取句子，打印句子的前10个 token，而不是词 ###\n",
      "我是一个小老虎，天天开心。\n",
      "\n",
      "\n",
      "他是一个【大笨猪】，走在大路\n",
      "\n",
      "\n",
      "哈哈哈，这是一个美妙的清晨，一切都\n",
      "走在张家界的山谷里，都是空气\n",
      "啦啦啦啦。\n",
      "快快走吧。\n",
      "\n",
      "快快乐乐吧...，一切都是快快乐乐...的\n",
      "\n",
      "\n",
      "快去吧，都是好好的。\n",
      "\n",
      " ##### 使用自定义分词器 ####\n",
      "who are you.\n",
      "\n",
      "give me... a token.\n",
      "\n",
      "this is... a notebook.\n",
      "\n",
      "hi give me five.\n"
     ]
    }
   ],
   "source": [
    "# 获取句子，打印句子的前3个 token\n",
    "print(\"\\n ###### 获取句子，打印句子的前10个 token，而不是词 ###\")\n",
    "for sent in doc4.sents:\n",
    "    print(sent[:10])\n",
    "\n",
    "# 自定义句子分隔符，进行句子检测\n",
    "print(\"\\n ##### 使用自定义分词器 ####\")\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "@Language.component(\"my_sentencizer\")\n",
    "def my_sentencizer(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == '...' and not token.is_punct and not token.is_sent_start:\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('my_sentencizer', before='parser')\n",
    "doc5 = nlp((pathlib.Path(\"./data/text.txt\").read_text(encoding='utf8')))\n",
    "for sent in doc5.sents:\n",
    "    print(sent)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T07:54:45.811058800Z",
     "start_time": "2024-09-09T07:54:44.757875200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gus', 'Proto', 'is', 'a', '【', 'Python', '】', 'developer', 'currently', 'working', 'for', 'a', 'London@based', 'Fintech', 'company', '.', 'He', 'is', 'interested', 'in', 'learning', 'Natural&Language&Processing', '.', '1234569', '.']\n"
     ]
    }
   ],
   "source": [
    "# 自定义分词器，将特殊字符，如：&、@等，能够分词出来，比如：new@year 可以分为两个token而不是一个\n",
    "\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a 【Python】 developer currently\"\n",
    "    \" working for a London@based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural&Language&Processing. 1234569. abced\"\n",
    ")\n",
    "\n",
    "print([token.text for token in nlp(custom_about_text)[:-1]])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T08:03:52.523484Z",
     "start_time": "2024-09-09T08:03:52.480578200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gus', 'Proto', 'is', 'a', '【', 'Python', '】', 'developer', 'currently', 'working', 'for', 'a', 'London', '@', 'based', 'Fintech', 'company', '.', 'He', 'is', 'interested', 'in', 'learning', 'Natural', '&', 'Language', '&', 'Processing']\n",
      "Gus 0\n",
      "Proto 4\n",
      "is 10\n",
      "a 13\n",
      "【 15\n",
      "Python 16\n",
      "】 22\n",
      "developer 24\n",
      "currently 34\n",
      "working 44\n",
      "for 52\n",
      "a 56\n",
      "London 58\n",
      "@ 64\n",
      "based 65\n",
      "Fintech 71\n",
      "company 79\n",
      ". 86\n",
      "He 88\n",
      "is 91\n",
      "interested 94\n",
      "in 105\n",
      "learning 108\n",
      "Natural 117\n",
      "& 124\n",
      "Language 125\n",
      "& 133\n",
      "Processing 134\n",
      ". 144\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
    "# 处理前面的标点符号（如左括号）的函数。\n",
    "prefix_re = spacy.util.compile_prefix_regex(\n",
    "    custom_nlp.Defaults.prefixes + [\"【\"]\n",
    ")\n",
    "# 处理后续标点符号（如右括号）的函数。\n",
    "suffix_re = spacy.util.compile_suffix_regex(\n",
    "    custom_nlp.Defaults.suffixes + ['】']\n",
    ")\n",
    "\n",
    "custom_infixes = [\"@\", \"&\"]\n",
    "\n",
    "# 处理非空格分隔符（如连字符）的函数。\n",
    "infix_re = spacy.util.compile_infix_regex(\n",
    "    list(custom_nlp.Defaults.infixes) + custom_infixes\n",
    ")\n",
    "\n",
    "# 自定义 tokenizer，其中token_match 用于匹配不应拆分的字符串。它会覆盖前面的规则，对 URL 或数字号码，等不应该拆分的实体很有用。\n",
    "custom_nlp.tokenizer = Tokenizer(\n",
    "    custom_nlp.vocab,\n",
    "    prefix_search=prefix_re.search,\n",
    "    suffix_search=suffix_re.search,\n",
    "    infix_finditer=infix_re.finditer,\n",
    "    token_match=None,\n",
    ")\n",
    "\n",
    "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
    "\n",
    "print([token.text for token in custom_tokenizer_about_doc[:-1]])\n",
    "\n",
    "for token in custom_tokenizer_about_doc:\n",
    "    print(token.text, token.idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T07:59:07.862986700Z",
     "start_time": "2024-09-09T07:59:07.113989900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n",
      "each\n",
      "eleven\n",
      "herself\n",
      "wherever\n",
      "ca\n",
      "few\n",
      "anywhere\n",
      "nevertheless\n",
      "with\n",
      "whereafter\n",
      "either\n",
      "might\n",
      "him\n"
     ]
    }
   ],
   "source": [
    "# stop_word 停用词，停用词通常定义为语言中最常见的词。在英语中，停用词的一些示例是 the、are、but 和 they。大多数句子需要包含停用词才能成为具有语法意义的完整句子。\n",
    "# 使用 NLP 时，停用词通常会被删除，因为它们并不重要，并且它们会严重扭曲任何词频分析。spaCy 存储英语的停用词列表：\n",
    "\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords)[:14]:\n",
    "    print(stop_word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T08:08:26.141879Z",
     "start_time": "2024-09-09T08:08:26.131907600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
     ]
    }
   ],
   "source": [
    "# 使用 is_stop属性删除停用词\n",
    "custom_about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_doc = nlp(custom_about_text)\n",
    "print([token for token in about_doc if not token.is_stop])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T08:13:36.258604600Z",
     "start_time": "2024-09-09T08:13:35.253479100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  is : be\n",
      "                  He : he\n",
      "               keeps : keep\n",
      "          organizing : organize\n",
      "             meetups : meetup\n",
      "               talks : talk\n"
     ]
    }
   ],
   "source": [
    "# 词性还原， 将 过去式 还原成 现在式\n",
    "# 词形还原是必需的，因为它可以帮助您减少单词的变形形式，以便可以将它们作为单个项目进行分析。它还可以帮助您规范化文本。\n",
    "\n",
    "# 不对文本进行词形还原，那么 organize 和 organizing 将被计为不同的标记，词形还原可帮助避免可能在概念上重叠的重复单词。进行词频统计。\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "conference_help_text = (\n",
    "    \"Gus is helping organize a developer\"\n",
    "    \" conference on Applications of Natural Language\"\n",
    "    \" Processing. He keeps organizing local Python meetups\"\n",
    "    \" and several internal talks at his workplace.\"\n",
    ")\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "    if str(token) != str(token.lemma_):\n",
    "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T08:17:28.677337900Z",
     "start_time": "2024-09-09T08:17:27.918782100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "# 词频统计， 统计文本中出现最多得词汇，初步分析文本主旨，此过程可能会丢失信息\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "complete_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech company. He is\"\n",
    "    \" interested in learning Natural Language Processing.\"\n",
    "    \" There is a developer conference happening on 21 July\"\n",
    "    ' 2019 in London. It is titled \"Applications of Natural'\n",
    "    ' Language Processing\". There is a helpline number'\n",
    "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
    "    \" He keeps organizing local Python meetups and several\"\n",
    "    \" internal talks at his workplace. Gus is also presenting\"\n",
    "    ' a talk. The talk will introduce the reader about \"Use'\n",
    "    ' cases of Natural Language Processing in Fintech\".'\n",
    "    \" Apart from his work, he is very passionate about music.\"\n",
    "    \" Gus is learning to play the Piano. He has enrolled\"\n",
    "    \" himself in the weekend batch of Great Piano Academy.\"\n",
    "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
    "    \" of London and has world-class piano instructors.\"\n",
    ")\n",
    "complete_doc = nlp(complete_text)\n",
    "\n",
    "words = [\n",
    "    token.text\n",
    "    for token in complete_doc\n",
    "    if not token.is_stop and not token.is_punct\n",
    "]\n",
    "\n",
    "print(Counter(words).most_common(5))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T08:19:48.811722700Z",
     "start_time": "2024-09-09T08:19:47.851745700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOKEN: Gus\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Proto\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: Python\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: developer\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: currently\n",
      "=====\n",
      "TAG: RB         POS: ADV\n",
      "EXPLANATION: adverb\n",
      "\n",
      "TOKEN: working\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: for\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: a\n",
      "=====\n",
      "TAG: DT         POS: DET\n",
      "EXPLANATION: determiner\n",
      "\n",
      "TOKEN: London\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: -\n",
      "=====\n",
      "TAG: HYPH       POS: PUNCT\n",
      "EXPLANATION: punctuation mark, hyphen\n",
      "\n",
      "TOKEN: based\n",
      "=====\n",
      "TAG: VBN        POS: VERB\n",
      "EXPLANATION: verb, past participle\n",
      "\n",
      "TOKEN: Fintech\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: company\n",
      "=====\n",
      "TAG: NN         POS: NOUN\n",
      "EXPLANATION: noun, singular or mass\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n",
      "\n",
      "TOKEN: He\n",
      "=====\n",
      "TAG: PRP        POS: PRON\n",
      "EXPLANATION: pronoun, personal\n",
      "\n",
      "TOKEN: is\n",
      "=====\n",
      "TAG: VBZ        POS: AUX\n",
      "EXPLANATION: verb, 3rd person singular present\n",
      "\n",
      "TOKEN: interested\n",
      "=====\n",
      "TAG: JJ         POS: ADJ\n",
      "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
      "\n",
      "TOKEN: in\n",
      "=====\n",
      "TAG: IN         POS: ADP\n",
      "EXPLANATION: conjunction, subordinating or preposition\n",
      "\n",
      "TOKEN: learning\n",
      "=====\n",
      "TAG: VBG        POS: VERB\n",
      "EXPLANATION: verb, gerund or present participle\n",
      "\n",
      "TOKEN: Natural\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Language\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: Processing\n",
      "=====\n",
      "TAG: NNP        POS: PROPN\n",
      "EXPLANATION: noun, proper singular\n",
      "\n",
      "TOKEN: .\n",
      "=====\n",
      "TAG: .          POS: PUNCT\n",
      "EXPLANATION: punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# 词性标记，根据每个词标记在句子中的用法为每个标记分配 POS 标签\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "about_text = (\n",
    "    \"Gus Proto is a Python developer currently\"\n",
    "    \" working for a London-based Fintech\"\n",
    "    \" company. He is interested in learning\"\n",
    "    \" Natural Language Processing.\"\n",
    ")\n",
    "about_doc = nlp(about_text)\n",
    "for token in about_doc:\n",
    "    print(\n",
    "        f\"\"\"\n",
    "TOKEN: {str(token)}\n",
    "=====\n",
    "TAG: {str(token.tag_):10} POS: {token.pos_}\n",
    "EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-09T08:30:28.378551200Z",
     "start_time": "2024-09-09T08:30:25.740559500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
